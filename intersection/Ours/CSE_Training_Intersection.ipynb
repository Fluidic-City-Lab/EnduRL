{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498f3f3d",
   "metadata": {},
   "source": [
    "## Congestion Statge Classifier (CSC) Training in Intersection.\n",
    "\n",
    "__The CSC predicts (based on current observation), what is be the expected traffic state 10-timesteps (1 second) into the future.__\n",
    "\n",
    "__1. Training:__\n",
    "\n",
    "    - Data collection: Data is collected using the RL training scripts but horizon is kept at 0 i.e., all vehicles inluding RL behave as IDM. The data is only collected for vehicles moving in the North-South direction because that is where it will be used.\n",
    "    \n",
    "    - The data is collected at same initial configuration of vehicles i.e., equally spaced. However, for variation the inflow is changed between 1000 veh/hr to 1600 veh/hr.\n",
    "    \n",
    "    - Total of 500 simulation runs are performed each with a warmup time of 800 and horizon of 0. It was observed that validation accuracy gets better with more data and 500 is enough.\n",
    "    \n",
    "__2. Neural Network configuration:__\n",
    "    \n",
    "    - Cannot be too large as that would create a lag in the system, especially when RV penetration is high.\n",
    "    - Same CSC neural network configuration is shared by all RVs.\n",
    "    \n",
    "__3. Data Processing:__\n",
    "\n",
    "    - The collected data mostly contains non-transition data (i.e., between current timestep and future timestep the state is the same).\n",
    "    - It is filtered and made equally representative of transition and non-transition (50% each).\n",
    "    - Further, the data is also made equally representative of the all  6 class labels.\n",
    "   \n",
    "__Additional Notes:__\n",
    "- Each data point has 3 things: The current timestep, current timestep monotonicity based label, and the feature set\n",
    "- The feature set looks like [[x1, v1],[x2, v2]...] normalized position and velocity of leading vehicles starting and including RL (default values -1 if no vehicle present).\n",
    "\n",
    "- Only 1 RL agent is collecting data at a time (Make data collection simpler). In the ring, the same RL vehicle would be collecting data for the entire horizon but here, rl vehicles are coming in and out of the network.    \n",
    "- In the intersection config file set the following params: HORIZON = 0, WARMUP = 1000, render = True, N_CPUS = 1, rv_penetration = 0.1 \n",
    "- To get a good balance of transition and non-transition data on all classes, periodically change inflow values to 800, 1000, 1300, 1500, 1600, 1800\n",
    "\n",
    "\n",
    "__Important: he No vehicles in front case.__\n",
    "\n",
    "- It does not make much sense to have \"no vehicles in front\" to have a transition \n",
    "- Based on the information that at present timestep there are no vehicles in front, you cannot predict the future\n",
    "- The control action is also deterministic in this case (accelerate to reach speed limit)\n",
    "- So, generate some synthetic data to represent this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326771a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSC Data generation/ collection\n",
    "# !python train.py intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba861cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c131932",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir('./csc_data')\n",
    "\n",
    "npy_files = []\n",
    "for folder in folders: \n",
    "    \n",
    "    # filter to ignore files\n",
    "    if \".npy\" not in folder: \n",
    "        npy_files_this_folder = [f'./csc_data/{folder}/' + file for file in os.listdir(f'./csc_data/{folder}') if file.endswith('.npy')]\n",
    "        print(f\"Folder: {folder}, npy_files_this_folder: {len(npy_files_this_folder)}\\n\")\n",
    "        npy_files.extend(npy_files_this_folder)\n",
    "\n",
    "print(f\"Total npy_files: {len(npy_files)}\")\n",
    "print(f\"First 2 files: {npy_files[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4eb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for file in npy_files: \n",
    "    data = np.load(file, allow_pickle=True)\n",
    "    all_data.extend(data)\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "print(f\"all_data.shape: {all_data.shape}, each data point shape: {all_data[0].shape}\")\n",
    "print(f\"Sample data: {all_data[0]}\") \n",
    "\n",
    "# [:,0] is timesteps\n",
    "# [:,1] is labels\n",
    "# [:,2] is observations\n",
    "\n",
    "print(f\"first 5 labels: {all_data[:5,1]}\") #all_data[:,1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME_WINDOW\n",
    "TIMESTEP_OFFSET = 10\n",
    "\n",
    "X = all_data[:, 2] # Observations\n",
    "y = all_data[:, 1] # TSE labels\n",
    "\n",
    "print(f\"Before offset, X: {X.shape}, y:{y.shape}\")\n",
    "\n",
    "# Go from 0 to (last - TIME_OFFSET)\n",
    "X = X[0: X.shape[0]-TIMESTEP_OFFSET]\n",
    "\n",
    "# Go from 0 to (last - TIME_OFFSET)\n",
    "y_current = y[0: y.shape[0]-TIMESTEP_OFFSET]\n",
    "\n",
    "# Go from 10 to last\n",
    "y_future = y[TIMESTEP_OFFSET:]\n",
    "      \n",
    "print(f\"After offset, X: {X.shape}, y_current:{y_current.shape}, y_future:{y_future.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a34580",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "# 0 = Leaving, 1 = Forming, 2 = Free Flow, 3 = Congested, 4 undefined, 5 No vehicle in front\n",
    "label_meanings = [\"Leaving\", \"Forming\", \"Free Flow\", \"Congested\", \"undefined\", \"No vehicle in front\"]\n",
    "\n",
    "def display_counts(indices, labels):\n",
    "    \n",
    "    num_non_transition = sum(len(indices['non-transition'][label]) for label in labels)\n",
    "    num_transition = sum(len(indices['transition'][label]) for label in labels)\n",
    "\n",
    "    print(f'Number of non-transitions: {num_non_transition}')\n",
    "    print(f'Number of transitions: {num_transition}')\n",
    "    print(f\"In percentage Transitions are: {round(100*(num_transition/num_non_transition),2)}%\\n\")\n",
    "\n",
    "    # Count the occurrences of each label\n",
    "    print(f'Non-transition ({num_non_transition}) counts by label:')\n",
    "    for label in sorted(labels):\n",
    "        this_count = len(indices['non-transition'][label])\n",
    "        print(f'\\t{label_meanings[label]}: {this_count}, percentage: {round(100*(this_count/num_non_transition),2)}%')\n",
    "\n",
    "    print(f'\\nTransition ({num_transition}) counts by label:')\n",
    "    for label in sorted(labels):\n",
    "        this_count = len(indices['transition'][label])\n",
    "        print(f'\\t{label_meanings[label]}: {this_count}, percentage: {round(100*(this_count/num_transition),2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two objectives:\n",
    "# Transition and non-transition data must be balanced\n",
    "# The 6 (5 in actual) labels must be balanced\n",
    "                  \n",
    "indices = {'transition': {key: [] for key in labels},\n",
    "           'non-transition': {key: [] for key in labels}}\n",
    "\n",
    "# If y_future and y_current have same label, its non-transition data\n",
    "# If they have different labels, its transition data\n",
    "for i, (current, future) in enumerate(zip(y_current, y_future)):\n",
    "    # non-transition\n",
    "    if current == future:  \n",
    "        indices['non-transition'][current].append(i)\n",
    "    # transition\n",
    "    else:  \n",
    "        indices['transition'][future].append(i)\n",
    "\n",
    "display_counts(indices, labels)\n",
    "\n",
    "# Perform selection\n",
    "# Whichever among all (transition and non-trainsition) has the lowest counts, pick everything to be that number\n",
    "actual_labels = [0, 1, 2, 3, 4] # Exclude No vehicle in front for the next couple of steps\n",
    "\n",
    "lowest_transition = min(len(indices['transition'][label]) for label in actual_labels)\n",
    "lowest_non_transition = min(len(indices['non-transition'][label]) for label in actual_labels)\n",
    "lowest = min(lowest_transition, lowest_non_transition)\n",
    "\n",
    "print(f\"\\nLowest: {lowest} data points\\n\")\n",
    "\n",
    "# select randomly\n",
    "selected_indices = {'transition': {key: [] for key in actual_labels},\n",
    "                    'non-transition': {key: [] for key in actual_labels}}\n",
    "\n",
    "for kind in ['transition', 'non-transition']:\n",
    "    for label in actual_labels:\n",
    "        if indices[kind][label]:\n",
    "            # Randomly select and add\n",
    "            selected_indices[kind][label] = random.sample(indices[kind][label], lowest) \n",
    "\n",
    "display_counts(selected_indices, actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef366c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same number (as lowest) of synthetic data for No vehicles in front\n",
    "# The first vehicle is RL (itself) will have some data\n",
    "\n",
    "no_veh_data = np.full((lowest, 20), -1.0)\n",
    "for i in range(lowest):\n",
    "    # Generate two random numbers between 0 and 1 and fill\n",
    "    random_numbers = np.random.rand(2)\n",
    "    no_veh_data[i, :2] = random_numbers\n",
    "    \n",
    "no_veh_labels = np.full((lowest,), 5)\n",
    "print(no_veh_data.shape, no_veh_labels.shape)\n",
    "print(no_veh_data[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ae132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the corresponsing observations as well\n",
    "X_dataset = []\n",
    "y_dataset = []\n",
    "\n",
    "# add real data\n",
    "for kind in ['transition', 'non-transition']:\n",
    "    for label in actual_labels:\n",
    "        for index in selected_indices[kind][label]:\n",
    "            X_dataset.append(X[index].flatten())\n",
    "            y_dataset.append(y_future[index])\n",
    "    \n",
    "X_dataset = np.array(X_dataset)\n",
    "y_dataset = np.array(y_dataset)\n",
    "\n",
    "#add synthethic data. No need to add data.\n",
    "X_dataset = np.append(X_dataset, no_veh_data, axis=0)\n",
    "y_dataset = np.append(y_dataset, no_veh_labels, axis=0)\n",
    "\n",
    "print(X_dataset.shape, y_dataset.shape)\n",
    "\n",
    "# Validation that the data operation was correctly done\n",
    "# This works, do not remove\n",
    "# Select a few indices from here\n",
    "# for i in actual_labels:\n",
    "#     print(selected_indices['non-transition'][i][0:2])\n",
    "#     print(selected_indices['transition'][i][0:2])\n",
    "# random_indices = [271567, 7646, 199950, 144989]\n",
    "\n",
    "# for i in random_indices:\n",
    "#     print(f\"\\n\\n{i}\")\n",
    "#     data =X[i].flatten()\n",
    "#     print(f\"Observation:{data, data.shape}\")\n",
    "#     print(f\"Future Label:\\t{y_future[i]}\")\n",
    "    \n",
    "#     # Find the index of 'data' in 'X_dataset'\n",
    "#     match_index = None\n",
    "#     for j, x in enumerate(X_dataset):\n",
    "#         if np.array_equal(x, data):\n",
    "#             match_index = j\n",
    "#             break\n",
    "#     print(f\"Match Index in X_dataset: {match_index}\")\n",
    "#     print(f\"Found in Dataset X: {X_dataset[match_index]}\\nWith Label: {y_dataset[match_index]}\\n\")\n",
    "# X_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaization of the observations (position, velocity), cluster of the 5 categories\n",
    "\n",
    "# Reshape each (10, 2) array into a 20-element vector\n",
    "X_flattened = X_dataset #np.array([x.flatten() for x in X_dataset])\n",
    "\n",
    "# Use t-SNE to reduce dimensionality to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X_flattened) # if t-SNE is slow; only use a subset\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters= 6, random_state=42)\n",
    "kmeans.fit(X_reduced)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Plot the data, colored by cluster assignment\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-means Clustering with t-SNE')\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.colorbar(label='Cluster label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train, val, and test (70%, 20%, 10%)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X_dataset, y_dataset, test_size=0.3, random_state=SEED)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=SEED)\n",
    "\n",
    "# # Printing the shapes of the resulting datasets\n",
    "# print(f\"Training set: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "# print(f\"Validation set: X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "# print(f\"Test set: X_test: {X_test.shape}, y_val: {y_test.shape}\")\n",
    "\n",
    "# Considering we have small amount of data, split into train, val=test (85%, 15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dataset, y_dataset, test_size=0.15, random_state=SEED, stratify=y_dataset)\n",
    "X_test = X_val\n",
    "y_test = y_val\n",
    "\n",
    "# Printing the shapes of the resulting datasets\n",
    "print(f\"Training set: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Validation set: X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Test set: X_test: {X_test.shape}, y_val: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1e523",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PT helpers\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class TSE_Net(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(TSE_Net, self).__init__() \n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b9f88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, criterion, optimizer, print_every):\n",
    "    net.train() \n",
    "    running_loss = 0.0\n",
    "    track_running_loss = []\n",
    "    for i, bdata in enumerate(train_loader, 0):\n",
    "        inputs, labels = bdata\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            running_loss = running_loss / print_every\n",
    "            track_running_loss.append(running_loss)\n",
    "            #print(f\"Training - Batch: {i+1}, Loss: {running_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return track_running_loss \n",
    "\n",
    "def validate(net, val_loader, criterion):\n",
    "    net.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for val_data in val_loader:\n",
    "            val_inputs, val_labels = val_data\n",
    "            val_outputs = net(val_inputs)\n",
    "            val_batch_loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += val_batch_loss.item()\n",
    "\n",
    "            # accuracy\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb73ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # 20\n",
    "num_classes = len(np.unique(y_dataset))  # 6\n",
    "net = TSE_Net(input_size, num_classes)\n",
    "\n",
    "n_epochs = 500\n",
    "batch_size = 32\n",
    "print_every = 50\n",
    "\n",
    "print(f\"The model has {count_parameters(net):} trainable parameters\")\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "train_data = TrafficDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                             torch.tensor(y_train.astype(int), dtype=torch.long))\n",
    "\n",
    "val_data = TrafficDataset(torch.tensor(X_val, dtype=torch.float32), \n",
    "                           torch.tensor(y_val.astype(int), dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_loss_values = []\n",
    "epoch_train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\", end = \"\\t\")\n",
    "    track_running_loss = train(net, train_loader, criterion, optimizer, print_every)\n",
    "    val_loss = validate(net, val_loader, criterion)\n",
    "    \n",
    "    val_loss_values.append(val_loss)\n",
    "    train_loss_values.extend(track_running_loss)\n",
    "    epoch_train_loss_values.append(np.mean(track_running_loss))\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "        save_path = './saved_models/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        torch.save(net.state_dict(), os.path.join(save_path, 'best_cse_model.pt'))\n",
    "        \n",
    "        print(\"###### New model saved ######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aba21e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_values, label = \"Training loss\")\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Running training loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3052bd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(val_loss_values, label=\"Validation loss\")\n",
    "plt.plot(epoch_train_loss_values, label=\"Avg Epoch loss at Training\")\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Per epoch loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29419d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = './saved_models/best_cse_model_intersection.pt'\n",
    "\n",
    "input_size = 10*2\n",
    "num_classes = 6\n",
    "\n",
    "saved_best_net = TSE_Net(input_size, num_classes)\n",
    "saved_best_net.load_state_dict(torch.load(best_model_path))\n",
    "saved_best_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_input = torch.randn(10, 2).flatten()\n",
    "# print(random_input)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = saved_best_net(random_input.unsqueeze(0))\n",
    "\n",
    "# _, predicted_label = torch.max(outputs, 1)\n",
    "# predicted_label = predicted_label.numpy()\n",
    "\n",
    "# print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze: In the validation set what did the best model get wrong?\n",
    "# Perform this on the test set\n",
    "\n",
    "label_meanings = [\"Leaving\", \"Forming\", \"Free Flow\", \"Congested\", \"undefined\", \"No vehicle in front\"]\n",
    "\n",
    "# Get confusion matrix \n",
    "y_test_pred = []\n",
    "with torch.no_grad():\n",
    "    for x in X_test:\n",
    "        outputs = saved_best_net(torch.from_numpy(x).float().unsqueeze(0)) # Dataloader makes it float at training\n",
    "        _, predicted_label = torch.max(outputs, 1)\n",
    "        y_test_pred.append(predicted_label.numpy()[0])\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = np.mean(y_test_pred == y_test)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "fs = 16\n",
    "fig, ax = plt.subplots(figsize=(7,7), dpi = 100)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fs-4)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", \n",
    "            cmap='Blues', \n",
    "            xticklabels=label_meanings, \n",
    "            yticklabels=label_meanings)\n",
    "\n",
    "ax.set_title('Confusion matrix', fontsize=fs)\n",
    "ax.set_xlabel('Predicted label',fontsize=fs-2)\n",
    "ax.set_ylabel('True label', fontsize=fs-2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f08078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To decide how many timesteps into the figure to use\n",
    "# Look at the error rate (test accuracy) vs timestep number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
